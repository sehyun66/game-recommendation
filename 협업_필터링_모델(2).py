# -*- coding: utf-8 -*-
"""협업 필터링 모델(2)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M1gEO8qZKRwSnQ6OBwxbmYSwOlQmnCc5
"""

# Commented out IPython magic to ensure Python compatibility.
## 코랩을 사용할 때
#드라이브 마운트
from google.colab import drive
drive.mount('/content/drive')

#현재 작업 위치 이동
#띄어쓰기에 \붙일 것
# %cd /content/drive/MyDrive/MultiCampus/데이터 시각화3

import pandas as pd

import pickle

df_steam_meta_con = pd.read_pickle('steam_meta.pkl')# 피클로 저장된 파일 불러오기

df_steam_meta_con.head()

!pip install surprise

from surprise import SVD
from surprise import Dataset
from surprise import Reader
from surprise.model_selection import cross_validate
from surprise import accuracy
from surprise import NMF

reader=Reader(rating_scale=(0,10))
data = Dataset.load_from_df(df_steam_meta_con[['appid','user','userscore']],
                           reader=reader)

cross_validate(SVD(),data,measures=['RMSE','MAE'],cv=5,verbose=1)

from sklearn.decomposition import TruncatedSVD

from tqdm import tqdm

SVD2 = TruncatedSVD(n_components=20)

pdf_steam= pd.pivot_table(df_steam_meta_con,
                     index='appid',
                     columns='user',
                     values='userscore'
)

matrix=SVD2.fit_transform(pdf_steam.fillna(0))
matrix.shape

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

similarity_rate=cosine_similarity(matrix,matrix)

np.fill_diagonal(similarity_rate,0)
similarity_rate

pdf_steam2=pdf_steam.reset_index()
game_title=pdf_steam2['appid']

similarity_rate2=pd.DataFrame(data=similarity_rate,index=game_title,columns=game_title)
similarity_rate2.head()

def recommend_game(appid, top_n):
   rec_game=similarity_rate2[appid].reset_index()
   rec_game = rec_game.rename(columns={"appid": "game",appid:'similarity'})
   rec_game=rec_game.sort_values(by='similarity',ascending=False).head(top_n)

   return rec_game

recommend_game(240,top_n=10)

# game1, game2, game3 추천 게임 - 유사도 테이블
def gentleman_ver2(game1, game2, game3):
  d1 = pd.DataFrame(recommend_game(game1, top_n=10), columns=['game', 'similarity']) # game1 추천목록 & 유사도
  d2 = pd.DataFrame(recommend_game(game2, top_n=10), columns=['game', 'similarity']) # game2 추천목록 & 유사도
  d3 = pd.DataFrame(recommend_game(game3, top_n=10), columns=['game', 'similarity']) # game3 추천목록 & 유사도
  df = pd.merge(d1, d2, on='game', how='outer') # d1, d2 merge
  df = pd.merge(df, d3, on='game', how='outer') # d2, d3 merge

  for game in df['game']: # ['game'] 행 하나씩 돌면서 
   if game in [game1, game2, game3]: # game이 game1, game2, game3에 해당하는 경우
      drop_index = df[df['game']==game].index
      df.drop(drop_index, axis='index', inplace=True) # 해당 행 drop

  # 추천 리스트 간 중복되는 게임이 없는 경우 각 추천리스트에서 2개씩 뽑음
  if len(df) == 30: 
    return [x for x in  recommend_game(game1, top_n=2)['game'].unique()]+ [x for x in recommend_game(game2, top_n=2)['game'].unique()] 
    + [x for x in recommend_game(game3, top_n=2)['game'].unique()]
  # 유사도 평균 내서 상위 6개 추출
  else : 
    df.fillna(0, inplace=True)
    df['mean'] = df.mean(axis=1, numeric_only=True)
    df.sort_values('mean', ascending=False, inplace=True)
    return list(df['game'][:6])

gentleman_ver2(240,3010,1225780)

from surprise.model_selection import GridSearchCV

from surprise import Dataset, Reader

trainset = data.build_full_trainset()
testset = trainset.build_testset()

param_grid = {'n_epochs': [5, 10, 15], 'lr_all': [0.002, 0.004, 0.006],
              'reg_all': [0.4, 0.6, 0.8]}
gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=5)

gs.fit(data)

# best RMSE score
print(gs.best_score['rmse'])

# combination of parameters that gave the best RMSE score
print(gs.best_params['rmse'])

print(gs)

joblib.dump(gs, 'gs.pkl')

df_gs=joblib.load("gs.pkl")

df_gs.best_score['rmse'], df_gs.best_params['rmse']

!pip install bayesian-optimization

from bayes_opt import BayesianOptimization
from surprise.model_selection import PredefinedKFold
from surprise.model_selection import KFold

def svd(n_factors, init_mean, init_std_dev, lr_all, reg_all ):
	params = {'n_factors' : n_factors,
		 'init_mean': init_mean, 'init_std_dev': init_std_dev,
		 'lr_all': lr_all, 'reg_all':reg_all}

	svd_algo = SVD(n_factors= int(params['n_factors']), n_epochs=10,
			init_mean= params['init_mean'],
			init_std_dev=params['init_std_dev'],
			lr_all=params['lr_all'],
			reg_all=params['reg_all'])
	
	rmse=cross_validate(svd_algo, data, measures=['RMSE'], cv=5)
	mean_rmse = rmse['test_rmse'].mean()
	return 1-mean_rmse

svd_Bay=BayesianOptimization(svd, {'n_factors':(int(150), int(180)),
				'init_mean':(0,0.65),
				'init_std_dev':(0.1,0.3),
				'lr_all':(0.015,0.035),
				'reg_all':(0.05,0.085)})
svd_Bay.maximize(init_points=5, n_iter=20, xi=0.02)
print('-' * 50)
print('Final Results')
print(svd_Bay.max)

svd_Bay

import joblib

joblib.dump(svd_Bay, 'svd_bay.pkl')

df_svd = joblib.load('svd_bay.pkl')
df_svd

print(df_svd.max)



